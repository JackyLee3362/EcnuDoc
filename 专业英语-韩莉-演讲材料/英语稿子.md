# DDPM

## 第1页：论文标题 + 作者名字

This paper presents progress in diffusion probabilistic models. 

本文介绍了扩散概率模型的研究进展



which we will call a “diffusion model” for brevity

为简洁起见，我们称之为“扩散模型”

## 第2页：介绍自己及组员

介绍自己 + 组员



## 第3页：什么是扩散模型 （What is Diffusion Model）

A diffusion model is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. 

扩散模型是一个参数化的马尔可夫链，使用变分推理，用于在有限时间后生成与数据匹配的样本。



Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. 

此链的过渡学会了逆转扩散过程，这是一个马尔可夫链，逐渐向采样方向相反的数据，直到信号被破坏。



When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.

当扩散包括少量高斯噪声，将采样链跃迁设置为条件就足够了高斯也是如此，允许特别简单的神经网络参数化



## 第二段

Diffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples. 

扩散模型易于定义且训练效率高，但据我们所知，没有证据表明它们能够产生高质量的样品

We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models. 

我们表明扩散模型实际上能够生成高质量的样本，有时优于其他类型的生成模型的已发表结果



In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling . 

此外，我们表明扩散模型的某种参数化揭示了与去噪的等价性在训练期间和退火朗格文动力学的多个噪声水平上进行分数匹配在采样期间。



We obtained our best sample quality results using this parameterization, so we consider this equivalence to be one of our primary contributions.

我们用这个获得了最好的样品质量结果参数化，因此我们认为这种等效性是我们的主要贡献之一。



## 第三段

Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models (our models do, 

尽管样本质量高，但与其他基于可能性的模型相比，我们的模型没有竞争性的对数似然



however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching.

（然而，我们的模型确实具有比基于能量的模型和分数匹配产生的大型估计退火重要性抽样更好的对数似然



We find that the majority of our models’ lossless codelengths are consumed to describe imperceptible image details. 

我们发现，我们模型的大部分无损代码长度都用于描述难以察觉的图像细节



We present a more refined analysis of this phenomenon in the language of lossy compression, and we show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.

我们用有损压缩的语言对这种现象进行了更精细的分析，并且我们表明扩散模型的采样过程是一种渐进式解码，类似于沿位顺序的自回归解码，极大地概括了自回归模型通常可能发生的事情

## 结论 Conclusion

We have presented high quality image samples using diffusion models, and we have found connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive models, and progressive lossy compression. Since diffusion models seem to have excellent inductive biases for image data, we look forward to investigating their utility in other data modalities and as components in other types of generative models and machine learning systems.

我们使用扩散模型展示了高质量的图像样本，并且我们发现扩散模型和变分推理之间的联系，用于训练马尔可夫链、去噪分数匹配和退火朗格文动力学（以及扩展的基于能量的模型）、自回归模型和渐进式有损压缩。由于扩散模型似乎对图像数据具有出色的归纳偏差，我们期待研究它们在其他数据模式中的实用性，以及作为其他类型的生成模型和机器学习系统的组件。

Our work on diffusion models takes on a similar scope as existing work on other types of deep
generative models, such as efforts to improve the sample quality of GANs, flows, autoregressive
models, and so forth. Our paper represents progress in making diffusion models a generally useful
tool in this family of techniques, so it may serve to amplify any impacts that generative models have
had (and will have) on the broader world.

我们在扩散模型方面的工作与现有其他类型的深部工作范围相似生成模型，例如努力提高 GAN 的样本质量、流、自回归模型等。我们的论文代表了使扩散模型普遍有用的进展工具，因此它可能有助于放大生成模型的任何影响曾经（并将拥有）在更广阔的世界中。

Unfortunately, there are numerous well-known malicious uses of generative models. Sample generation techniques can be employed to produce fake images and videos of high profile figures for political purposes. While fake images were manually created long before software tools were available, generative models such as ours make the process easier. Fortunately, CNN-generated images currently have subtle flaws that allow detection [62], but improvements in generative models may make this more difficult. 

不幸的是，生成模型有许多众所周知的恶意使用。样本生成技术可用于生成高调人物的假图像和视频政治目的。虽然假图像早在软件工具可用之前就已经手动创建，但像我们这样的生成模型使这个过程更容易。幸运的是，CNN生成的图像目前存在允许检测的细微缺陷[62]，但生成模型的改进可能会使这更加困难。

Generative models also reflect the biases in the datasets on which they are trained. As many large datasets are collected from the internet by automated systems, it can be difficult to remove these biases, especially when the images are unlabeled. If samples from generative models trained on these datasets proliferate throughout the internet, then these biases will only be reinforced further.

生成模型还反映了它们所依赖的数据集中的偏差受过培训。由于许多大型数据集是由自动化系统从互联网上收集的，因此它可以很难消除这些偏差，尤其是当图像未标记时。如果来自生成式的样本在这些数据集上训练的模型在整个互联网上激增，那么这些偏差只会进一步加强。

On the other hand, diffusion models may be useful for data compression, which, as data becomes higher resolution and as global internet traffic increases, might be crucial to ensure accessibility of the internet to wide audiences. Our work might contribute to representation learning on unlabeled raw data for a large range of downstream tasks, from image classification to reinforcement learning, and diffusion models might also become viable for creative uses in art, photography, and music.

另一方面，扩散模型可能对数据压缩有用，随着数据成为更高的分辨率和全球互联网流量的增加，对于确保可访问性可能至关重要面向广大受众的互联网。我们的工作可能有助于无标记的表征学习从图像分类到强化学习等各种下游任务的原始数据，扩散模型也可能在艺术、摄影和音乐中的创造性用途中变得可行。



## 公式推导（王鑫轲）

In the process of diffusion, the parameter reformulation technique can be used. Thus, instead of iterating to find the sampling results at each step, all the sampling results can be found based on x0 and a random variable z satisfying a Gaussian distribution. The formula is derived as follows

在扩散的过程中，可以使用参数重整化技巧。从而不需要迭代去求每一步的采样结果，而是所有的采样结果都可以基于x0和满足高斯分布的随机变量z求得。公式推导过程如下

First, according to the previous noise-added formula, we can obtain the derivation of xt at each step, then make alpha equal to 1-beta, thus transforming from Eq. 1 to Eq. 2. Afterwards, using the derivation of xt-1, we can transform from Eq. 2 to Eq. 4. Afterwards, after the superposition of Gaussian distribution, we transform to Eq. 5.

首先根据前面加噪的的公式，我们可以得到xt每一步推导式，然后令alpha等于1-beta，从而由公式1变换到公式2。之后使用xt-1的推导式，由公式2转换到公式4。之后经过高斯分布的叠加，变换到公式5。

According to the generalization of the above simplification process, we keep simplifying and finally use x0 and the random variable z to represent xt. The distribution of xt under the final x0 condition satisfies the Gaussian distribution with mean sqrt(alpha_bar)x0 and variance 1-alpha_bar.

根据上面化简过程的推广，不断化简，最终使用x0和随机变量z来表示xt。最终x0条件下xt的分布满足均值为sqrt(alpha_bar)x0,方差为1-alpha_bar的高斯分布。

##  直观介绍DDPM

那我们来对这一模型原理进行一个感官上面的介绍，首先我们先来看 现实中的扩散过程，这是红色染料在水中的扩散，逐渐变得均匀分布，而对应到图形数据也是一样，我们可以通过一些方法对图形数据分布 进行破坏，然后得到一个均一分配的数据分布；然后我们想的是，是 否可以将这一个过程反过来，也就是从均匀分布反向变回某种有意义 的数据分布



那我们可以对这个染料不断放大，在微观尺度上，这些染料分子扩散 时都是在做无规则的布朗运动，它们的每一时刻的位置都遵循一个小 的高斯分布；有意思的是，如果我们把这个过程倒放，这一反向过程 同样也遵循高斯分布



基于这一点的启发，我们对接下来需要做的事情就有了一个大致的思 路，首先是通过扩散过程引入噪声破坏数据分布的结构；然后通过扩 散过程的逆过程同样引入一个噪声并建立模型来学习这个，也就是估 计这一过程中每一步数据分布的均值和方差 